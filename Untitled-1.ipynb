{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import pickle\n",
    "import time\n",
    "screenshot_path = './captured/tda_lab_member_page.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform OCR on the screenshot\n",
    "def perform_ocr(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "# Function to classify names using the loaded model\n",
    "def classify_names(names, vectorizer, model):\n",
    "    turkish_names = []\n",
    "    for name in names:\n",
    "        name_vectorized = vectorizer.transform([name])\n",
    "        is_turkish = model.predict(name_vectorized)[0] == 1\n",
    "        if is_turkish:\n",
    "            turkish_names.append(name)\n",
    "    return turkish_names\n",
    "\n",
    "# Load your trained model and vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "with open('logistic_regression_model.pkl', 'rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text = perform_ocr(screenshot_path)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "with open('extracted_text.txt', 'w') as file:\n",
    "    file.write(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Your text goes here\n",
    "# load the text from the file\n",
    "with open('extracted_text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract and print names\n",
    "extracted_names = set()  # Using a set to avoid duplicate names\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        extracted_names.add(ent.text.strip())\n",
    "\n",
    "names = [] \n",
    "# Display the extracted names, and convert these into title case\n",
    "for name in extracted_names:\n",
    "    names.append(name.title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jonathan Kho',\n",
       " 'Summer Intern',\n",
       " 'Jordan',\n",
       " 'Ben Cobb',\n",
       " '@/]\\\\B',\n",
       " 'Asst',\n",
       " 'Xusheng Wang',\n",
       " 'James Fox',\n",
       " 'Guangjun Xu',\n",
       " 'Kemal Eren',\n",
       " 'Mehmet Deveci',\n",
       " 'Jonathan Kho',\n",
       " 'Toulouse',\n",
       " 'Doruk Bozdag',\n",
       " 'Prabha Kumarasamy',\n",
       " 'Metin',\n",
       " 'Anas Abu-Doleh',\n",
       " 'Catalyiirek',\n",
       " 'Lingchen Xiong',\n",
       " 'Catalytrek',\n",
       " 'Umit Catalyurek',\n",
       " 'Anne Benoit',\n",
       " 'Onur Kucuktunc',\n",
       " 'Shiv Kumar',\n",
       " 'Mustafa Kemal Tas',\n",
       " 'Catalytirek',\n",
       " 'Olcay Sertel',\n",
       " 'Kasimir Gabert',\n",
       " 'S. Krishnamoorthy',\n",
       " 'James Fox',\n",
       " 'Zheng Zhou',\n",
       " 'Julien Herrmann',\n",
       " 'O. Bas',\n",
       " 'M. Yusuf Ozkaya',\n",
       " 'Zheng Zhou',\n",
       " 'Senturk',\n",
       " 'A. Kalyanaraman',\n",
       " 'Muge Kural',\n",
       " 'Tim Hartley',\n",
       " 'P. Sadayappan',\n",
       " 'Yusuf Ozkaya',\n",
       " 'Erik Saule',\n",
       " 'Catalyurek']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now from these, let's classify the Turkish names, if the names are not made up AT least two words, then we will remove them\n",
    "\n",
    "turkish_names = classify_names(names, vectorizer, clf)\n",
    "\n",
    "for name in turkish_names:\n",
    "    if len(name.split()) < 2:\n",
    "        turkish_names.remove(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kemal Eren',\n",
       " 'Mehmet Deveci',\n",
       " 'Doruk Bozdag',\n",
       " 'Onur Kucuktunc',\n",
       " 'Mustafa Kemal Tas',\n",
       " 'Olcay Sertel',\n",
       " 'M. Yusuf Ozkaya',\n",
       " 'Muge Kural',\n",
       " 'Yusuf Ozkaya']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turkish_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
